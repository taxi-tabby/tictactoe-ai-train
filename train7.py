import numpy as np
import tensorflow as tf
import os
import re
from tensorflow.keras.models import Sequential # type: ignore
from tensorflow.keras.layers import Conv2D, Flatten, Dense, Dropout, MaxPooling2D, BatchNormalization, LeakyReLU, Reshape # type: ignore
from tensorflow.keras.optimizers import Adam # type: ignore
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau # type: ignore
from tqdm import tqdm  
from utils import create_model1, dynamicBatchSize

# EarlyStopping ì„¤ì •
early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.0001)





# âœ… í´ë” ë‚´ì˜ íŒŒì¼ë“¤ì„ í™•ì¸í•˜ê³ , í•„ìš”í•œ ë°ì´í„°ë¥¼ ë¡œë”©
directory = './npy'
files = [f for f in os.listdir(directory) if f.endswith('.npy')]

# íŒ¨í„´ì„ ì´ìš©í•´ 'train_x'ì™€ 'train_y' íŒŒì¼ì„ ë§¤ì¹­
pattern = re.compile(r"train_(x|y)_(\d+)x(\d+)_(\d+)\.npy")

train_files = {'x': {}, 'y': {}}

# íŒŒì¼ ì´ë¦„ì—ì„œ x, y ë°ì´í„° ë¶„ë¦¬í•˜ì—¬ dictionaryì— ì €ì¥
for file in files:
    match = pattern.match(file)
    if match:
        file_type = match.group(1)  # 'x' ë˜ëŠ” 'y'
        x_size = int(match.group(2))  # x í¬ê¸°
        y_size = int(match.group(3))  # y í¬ê¸°
        win_length = int(match.group(4))  # y í¬ê¸°
        shape = (x_size, y_size, win_length)
        
        # xì™€ y íŒŒì¼ì„ êµ¬ë¶„í•˜ì—¬ ì €ì¥
        if file_type == 'x':
            train_files['x'][shape] = file
        elif file_type == 'y':
            train_files['y'][shape] = file

dataLengthCollector = []

# âœ… ë°°ì¹˜ í¬ê¸°ë¥¼ ë¯¸ë¦¬ êµ¬í•˜ê¸° ìœ„í•´ ë¨¼ì € ì ‘ê·¼í•´ì„œ ì‚¬ì´ì¦ˆë¥¼ ì €ì¥
for shape in tqdm(train_files['x'].keys(), desc="Collecting Data Lengths all training data"):
    x_file = train_files['x'].get(shape)
    y_file = train_files['y'].get(shape)
    
    train_x = np.load(os.path.join(directory, x_file))
    data_size = train_x.shape[0]
    
    dataLengthCollector.append(data_size)



print(f"ğŸ”¹ ----------------------------------------------------------------------------------")
print(f"ğŸ”¹ training size: {dataLengthCollector}:")


# âœ… ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ë£¨í”„
for shape in tqdm(train_files['x'].keys(), desc="Training Models"):

    x_file = train_files['x'].get(shape)
    y_file = train_files['y'].get(shape)
    win_length = shape[2]
    
    # xì™€ y íŒŒì¼ì´ ëª¨ë‘ ì¡´ì¬í•˜ëŠ” ê²½ìš°ì—ë§Œ ì§„í–‰
    if x_file and y_file:
        
        # âœ… Numpy íŒŒì¼ ë¡œë“œ
        train_x = np.load(os.path.join(directory, x_file))
        train_y = np.load(os.path.join(directory, y_file))

        # âœ… ë™ì ìœ¼ë¡œ ë³´ë“œ í¬ê¸° ì¶”ì¶œ
        data_size, x_size, y_size = train_x.shape[0], train_x.shape[1], train_x.shape[2]  # x_size, y_sizeë¥¼ ë™ì ìœ¼ë¡œ ì¶”ì¶œ

        batch_size = dynamicBatchSize(dataLengthCollector, data_size)

        train_x_shape = np.expand_dims(train_x, axis=-1)         
        train_y_int = np.argmax(train_y, axis=1)


        # âœ… CNN ì…ë ¥ í˜•ì‹ì— ë§ê²Œ reshape
        # train_x = train_x.astype('float32').reshape(-1, shape[0], shape[1], 1)  # CNN ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜
        # train_xì™€ train_yëŠ” ì´ë¯¸ One-Hot ì¸ì½”ë”© ë˜ì–´ ìˆìŒ. ì¶”ê°€ ë³€í™˜ í•„ìš” ì—†ìŒ.

        # âœ… train_xì™€ train_yì˜ ê¸¸ì´ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
        if train_x.shape[0] != train_y.shape[0]:
            print(f"âŒ train_x and train_y lengths do not match for shape {shape}: {train_x.shape[0]} vs {train_y.shape[0]}")
            continue  # ì´ ê²½ìš° í•´ë‹¹ í¬ê¸° ëª¨ë¸ í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.

        print(f"ğŸ”¹ ----------------------------------------------------------------------------------")
        print(f"ğŸ”¹ Model Training for size {shape}:")
        print(f"   train_x shape: {train_x.shape}")  
        print(f"   train_y shape: {train_y.shape}")  
        print(f"   batch_size: {data_size} -> {batch_size}")

        # âœ… ëª¨ë¸ ìƒì„±
        input_shape = train_x_shape.shape[1:]  # (height, width, channels)

        # ë™ì ìœ¼ë¡œ ëª¨ë¸ ìƒì„±
        # model = create_improved_model(input_shape, train_y.shape[-1])  # num_classesëŠ” train_yì˜ ë§ˆì§€ë§‰ ì°¨ì›ì˜ í¬ê¸°
        model = create_model1(input_shape) 
        
        # ëª¨ë¸ í›ˆë ¨
        model.fit(train_x_shape, train_y_int, epochs=2000, batch_size=batch_size, validation_split=0.005, callbacks=[reduce_lr, early_stopping])
        
        # ëª¨ë¸ í‰ê°€
        loss, accuracy = model.evaluate(train_x_shape, train_y_int)
        print(f"âœ… Model loss: {loss}, accuracy: {accuracy}")

        # ëª¨ë¸ í‰ê°€ ê²°ê³¼ë¥¼ íŒŒì¼ì— ì €ì¥
        
        log_file = 'evaluate_log.txt'
        with open(log_file, 'r') as file:
            existing_logs = file.read()

        new_log = f"Model for size {shape} - Loss: {loss}, Accuracy: {accuracy}\n"

        with open(log_file, 'w') as file:
            file.write(new_log + existing_logs)
    

        # ëª¨ë¸ ì €ì¥
        model_dir = './model'
        if not os.path.exists(model_dir):
            os.makedirs(model_dir)

        model_filename = f"tictactoe_model_{shape[0]}x{shape[1]}_{win_length}.h5"
        model.save(os.path.join(model_dir, model_filename))
        print(f"ğŸ¯ Model saved as '{model_filename}'")
    
    else:
        print(f"âŒ Missing x or y data for shape {shape}. Skipping...")

print("ğŸ‰ All models have been trained and saved.")
